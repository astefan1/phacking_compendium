---
title: "P-Hacking Compendium Documentation"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{phackR_vignette_tmp}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup, message=FALSE}
library(phackR)
```

## Idea: A Compendium of P-Hacking Strategies
In the literature, different papers describe different p-hacking strategies. We assemble the p-hacking strategies that were mentioned by other authors and provide an R-package to simulate their effects on the results of analyses. Here, we describe the p-hacking strategies and the simulation functions in detail.

## General Remarks

### Structure of the code
Every p-hacking strategy is stored in a separate file. In each file, there are three main code blocks. The first block contains a function to simulate data that can be used with the p-hacking strategy (or a link to a more generic function that can be used, e.g., from the "helpers.R" file). The second block contains the p-hacking function. Here, we define the strategy to analyze the data with (different levels of) p-hacking. Typically, these functions take the data that were created in the first block as an input. The output is a list with two elements: `p.final`, i.e., the final p-hacked p-value, and `ps`, i.e., all p-values "visited" in the process. Typically the first ps value is the original, non-p-hacked p-value. The third block combines the functions from the first two code blocks in one simulation function. Essentially, the functions in this block consist of a big loop where data are created and analyzed repeatedly. The output of this function is always a matrix with two columns: The first column contains the p-hacked values, i.e., the `p.final` values from the p-hacking function. The second column contains the "raw" non p-hacked values. If the p-hacking was "successful" in the respective iteration, the first column will show a smaller p-value than the second column. If the p-hacking was "unsuccessful", the first and second column won't differ.

Data simulation functions, p-hacking functions, and any helper functions are specified as internal functions, i.e., start with a "." and are not exported (alas, we don't want people to use this as a toolkit for p-hacking, so let's use this as a safety belt...). All simulation functions are exported and start with "sim.". All functions are documented using `roxygen` documentation. Unit tests of the simulation functions can be found in the `tests` folder.

### Hypothesis tests
We use two kinds of hypothesis tests: the independent samples t-test and univariate linear regression. The standard choice is the independent samples t-test. However, some p-hacking methods are not easily applicable to this hypothesis test or are more likely to be used in a continuous variable case. In these cases, we use a univariate linear regression. Some p-hacking strategies specifically rely on different hypothesis tests. In these cases, this is mentioned in the description of the p-hacking strategies below.

The result object of the p-hacking simulation functions contains the original and p-hacked p-value for all analyses. Additionally, whenever possible the determination coefficient $R^2$ of the original and p-hacked result is shown as well. When only t-tests are used in the simulation, the output also contains the original and p-hacked Cohen's d. The "hacked" effect sizes are always based on the p-hacked p-values, i.e., the analysis is still selected based on the smallest p-value, not on the smallest effect size.

### "Normal" versus "Ambitious" p-hacking
Most p-hacking simulation functions have an argument "strategy". This argument determines how the final p-value is chosen. There are three options: `strategy = "firstsig"` simulates the situation where a researcher tries out some p-hacking methods and stops as soon as the result is significant, that is, at the first significant p-value. In a comment on Simonsohn et al. (2014), Ulrich & Miller (2015) argued that researchers might instead engage in "ambitious" p-hacking where the smallest significant p-value is selected. This strategy is implemented in the option `strategy = "smallest.sig"`. Simonsohn (private comm.) argues that there might exist a third p-hacking strategy where the researchers try a number of different analysis options and select the smallest p-value no matter if it is significant or not. We implemented this in the option `strategy = "smallest"`. This strategy implies that researchers use p-values as a criterion for validity of analyses "whichever analysis works better is probably the right analysis to keep" (Simonsohn, private comm.).

### True effect sizes
The true effect size in the simulations is always zero. This means that the data are simulated such that there is no difference between the means of the two groups in the t-test (mean = 0, sd = 1) or that the correlation between the two variables of interest is zero.

### Default settings
All p-hacking simulation functions come with the same default settings for common arguments. We will only explain these once and mention below if anything differs from these defaults.

* `strategy`: As described above, this argument defines the strategy of selecting p-values (`strategy = "firstsig"`), (`strategy = "smallest.sig"`), or (`strategy = "smallest"`). The default setting is `strategy = "firstsig"`.
* `alpha`: The significance level of the hypothesis test. If p < alpha, the result is deemed significant. The default setting is `alpha = 0.05`.
* `iter`: Number of iterations in the simulation process. The default is `iter = 1000`, i.e., the simulation will result in 1000 p-hacked p-values.
* `alternative`: Whenever a t-test is used as an analysis function, the argument `alternative` specifies the direction of the t-test. As in the `stats::t.test` function, the default is `two.sided`.

## Description of P-Hacking Strategies and Their Implementation

### Selective reporting of the dependent variable
We simulate this p-hacking strategy using the t-test. The **simulated dataset** contains one discrete group variable with two levels (independent variable) and several continous variables (dependent variables). The correlation between the dependent variables can be set using the argument `r`. The size of the groups can be defined by `nobs.group` (either an integer for equally sized groups or a vector with two elements for groups of different sizes). The **p-hacking strategy** is implemented as follows: Compute t-test with every single of the dependent variables. Then select the final p-value using one of the p-value selection strategies (first significant p-value, smallest significant p-value, smallest p-value). The output contains the original and p-hacked p-values, the original and p-hacked $R^2$, and the original and p-hacked Cohen's d.

The simulation code can be found in the file `selectiveReportingDV.R`.

**Example**

```{r selectiveReportingDV}
set.seed(1234)
sim.multDVhack(nobs.group = 30, nvar = 5, r = 0.3, strategy = "smallest", 
               iter = 10, alternative = "two.sided", alpha = 0.05)
```

### Selective reporting of the independent variable
We simulate this p-hacking strategy using the t-test. The **simulated dataset** contains one control group variable and multiple treatment group variables (independent variables), thus it is in a wide format. The correlation between the treatment group variables can be set using the argument `r`. The size of the groups can be defined by `nobs.group`. Note that unequal sizes of groups are not possible here because the data set is in wide format. The **p-hacking strategy** is implemented as follows: Conduct an independent samples t-test with every single treatment variable and the control group variable. Then select the final p-value using one of the p-value selection strategies (first significant p-value, smallest significant p-value, smallest p-value). The output contains the original and p-hacked p-values, the original and p-hacked $R^2$, and the original and p-hacked Cohen's d.

The simulation code can be found in the file `selectiveReportingIV.R`.

**Example**

```{r selectiveReportingIV}
set.seed(1234)
sim.multIVhack(nobs.group = 30, nvar = 5, r = 0.3, strategy = "smallest", 
               iter = 10, alternative = "two.sided", alpha = 0.05)
```

### Incorrect rounding
We simulate this p-hacking strategy using the t-test, but literally any hypothesis test should produce the same results. The **simulated dataset** contains one discrete group variable with two levels (independent variable) and one continous variable (dependent variable). Note that the simulation function does not include an argument for the group size as the group size is irrelevant in this p-hacking context. The **p-hacking strategy** is implemented as follows: Compute the p-value for the t-test, if it is between the alpha level and a rounding level specified in the argument `roundinglevel` (e.g., 0.06 > p > 0.05), round it down to p = 0.05, else keep the original p-value. Note that selecting a p-value selection strategy (first  significant p-value, smallest significant p-value, smallest p-value) is not possible for this p-hacking strategy. The output contains the original and p-hacked p-values, and the original and p-hacked $R^2$. Note that $R^2$ does not change because it is unlikely that researchers would adjust effect sizes retrospectively based on a rounded p-value.

The simulation code can be found in the file `incorrectRounding.R`.

**Example**

```{r incorrectRounding}
set.seed(1234)
sim.roundhack(roundinglevel = 0.06, iter = 10, alternative = "two.sided", 
              alpha = 0.05)
```

### Optional stopping / Data peeking
We simulate this p-hacking strategy using the t-test. The **simulated dataset** contains one discrete group variable with two levels (independent variable) and one continous variable (dependent variable), both should have at least the sample size defined in `n.max`. Note that this is the reason why the simulation function does not have an argument to specify the sample size per group (it is automatically set to n.max). The **p-hacking strategy** is implemented as follows: The dataset is evaluated row-by-row, starting with a minimum sample size of `n.min`. At each step, a number of observations is added to the sample, defined by the argument `step` and the t-test is computed. This continues until the maximum sample size specified in `n.max` is reached. The p-hacked p-value is defined as the first p-value that is smaller than the defined alpha level. The non-p-hacked p-value is defined as the p-value at n.max (this implies that if there was no p-hacking, n.max would be the specified fixed sample size). The output contains the original and p-hacked p-values, the original and p-hacked $R^2$, and the original and p-hacked Cohen's d.

The simulation code can be found in the file `optionalStopping.R`.

**Example**
```{r optionalStopping}
set.seed(1234)
sim.optstop(n.min = 10, n.max = 20, step = 2, alternative = "two.sided", 
            iter = 10, alpha = 0.05)
```

### Outlier exclusion
We simulate this p-hacking strategy using univariate linear regression (most outlier detection methods are based on some kind of distance measures on continuous variables). The **simulated dataset** contains two continuous variables, both with the sample size specified in the argument `nobs`. The **p-hacking strategy** is implemented as follows: The original p-value is computed as a simple linear regression(y ~ x) with all values. For the p-hacked p-values, outlier values are searched in x and y using an outlier definition criterion. Then, a pairwise deletion of outlier values follows for the following three options: (1) remove xy pairs where x is an outlier, (2) remove xy pairs where y is an outlier, (3) remove xy pairs where x and y are outliers. Therefore, for every outlier deletion strategy, we end up with three datasets where different values are removed (some of these may be identical and will be cut away by the `.extractoutlier` function in practice). The p-hacking method contains 12 separate outlier definition criteria, some of which have several sub-criteria (e.g., delete values that are > *x* standard deviations above or below the mean, with *x* being 2, 2.5, 3, ...). In the case of sub-criteria, the outlier deletion process is applied to each sub-criterion, so that we end up with *3 x number of sub-criteria* data sets (again, some of which might be identical and cut away by the `.extractoutlier` function). In the simulation function, either all separate outlier definition criteria can be applied (i.e., `which = c(1:12)`), a specific subset can be chosen (e.g., `which = c(1,3,5)`), or a random subset of 5 different outlier strategies can be chosen (`which = "random"`). We believe that the latter shows a realistic scenario as not all researchers know all outlier definition criteria and the ones they know are probably a random subset of the existing outlier definition criteria (which does not mean that our simulation method covers all existing outlier definition criteria). For all outlier definition criteria, the outlier-deleted datasets are created and evaluated using the t-test function. In the end, the final p-value is selected using one of the p-value selection strategies (first significant p-value, smallest significant p-value, smallest p-value). The output contains the original and p-hacked p-values as well as the original and p-hacked $R^2$.

Below, we describe all outlier exclusion criteria:

1. Boxplot: Single-variable outliers are defined using the `graphics::boxplot` function.
1. Stem-and-Leaf plot: Single-variable outliers are defined using the `aplpack::stemleaf` function.
1. Standard deviation: Single-variable outliers are defined as *x* standard deviations above or below the mean. The standard outlier definition rule is "a value is an outlier if it is more than 2 standard deviations above or below the mean". If there are outliers according to this definition, the standard deviation is increased in steps of 0.5 and in each step new datasets without outliers are created.
1. Percentage: Outliers are defined as the highest or lowest *x* percent of the values. As in the standard deviation definition, different values for x are tried out, starting from *1/sample size* (smallest sample quantile) to 0.05 in steps of 0.005.
1. Studentized residuals: Outliers are definetd as x or y values with high studentized residuals following from the regression y ~ x. If the largest absolute residual value is smaller than 2, the three largest residuals are marked as outliers. If the largest absolute residual value is larger than 2, different cutoff definitions are applied as with the standard deviation criterion.
1. Standardized residuals: Same outlier definition as with studentized residuals, just now the residual values are standardized.
1. DFBETA: DFBETA values measure the influence of values on the regression slope (y ~ x). The values with the highest 1-3 values are defined as outliers.
1. DFFITS: Values that have absolute DFFIT values larger than $2 \cdot \sqrt{2/n}$ are defined as outliers (see Wikipedia page for DFFITS for a justification of the cutoff).
1. Cook's distance: Values that have a Cook's distance larger than the median of an F distribution with *p = 2* and *n-p* degrees of freedom or larger than 1 are defined as outliers (see Wikipedia page for Cook's distance for the cutoff).
1. Mahalanobis distance: Values that have a high robust Mahalanobis distance ($Md^2 > \chi^2(0.98, 2)$) are defined as outliers (see Filzmoser et al., 2005).
1. Leverage values: Values that have high leverage values (3 times larger than the mean leverage value: $3 \cdot (p/n)$) are defined as outliers (see [here](https://newonlinecourses.science.psu.edu/stat501/node/338/ for the cutoff)). 
1. Covariance ratio: Values that have a covariance ratio differing from 1 are defined as outliers (see function `stats::influence.measures`).

The simulation code can be found in the file `outlierExclusion.R`.

**Example**
```{r outlierExclusion}
set.seed(1234)
sim.outHack(nobs = 30, which = "random", strategy = "smallest", alpha = 0.05, 
            iter = 10)
```


### Exploiting covariates
We simulate this p-hacking strategy using a t-test that is expanded to an ANCOVA when covariates are taken into account. The **simulated dataset** contains a discrete group variable with two levels (independent variable), a continuous dependent variable, and a set of continuous covariates. The number of covariates can be specified using the argument `ncov`. The correlation of the dependent variable with the covariates as well as the correlation among the covariates can be specified using the arguments `rcovdv` and `rcov`, respectively. The **p-hacking strategy** is implemented as follows: First, the original p-value without covariates is calculated. Then, all covariates are added to the model separately (i.e., dv ~ group + cov1, df ~ group + cov2, ...). Then, covariates are ordered in decreasing order with respect to their correlation with the dependent variable and then added to the model sequentially (i.e., dv ~ group + covhighest, dv ~ covhighest + covsecondhighest, ...). All models are evaluated and the final p-value is selected using one of the strategies (first significant p-value, smallest significant p-value, smallest p-value). The models can either be defined as models without interaction terms (`interactions = FALSE` as is the default) or with interaction terms. The output contains the original and p-hacked p-values as well as the $ partial\,\eta^2$. In this case, we do not display the overall $R^2$ of the analyses because in the ANCOVAs the focus does not lie on the overall explained variance.

The simulation code can be found in the file `exploitCovariates.R`.

**Example**
```{r exploitCovariates}
set.seed(1234)
sim.covhack(nobs.group = 30, ncov = 4, rcov = 0.3, rcovdv = 0.5, 
            interactions = FALSE, strategy = "smallest", 
            alpha = 0.05, iter = 10)
```

### Subgroup analyses / Tinkering with inclusion criteria
We simulate this p-hacking strategy using a t-test. The **simulated dataset** consists of one discrete group variable (independent variable), one continuous dependent variable, and several binary subgroup variables. The size of the original groups can be defined by `nobs.group` (either an integer for equally sized groups or a vector with two elements for groups of different sizes). Group sizes in the subgroup variables can differ (group membership is sampled randomly from `c(0,1)`). The number of subgroup variables can be defined using the argument `nsubvars`. The **p-hacking strategy** is implemented as follows: First, the original p-value is computed using the full dataset (independent and dependent variable). Then, for each subgroup variable, the dataset is split into two parts and the t-test is conducted separately in each part (e.g., as if one would conduct the same t-test for men and women). Then the final p-value is selected using one of the p-value selection strategies (first significant p-value, smallest significant p-value, smallest p-value). The output contains the original and p-hacked p-values, the original and p-hacked $R^2$, and the original and p-hacked Cohen's d.

Note: This p-hacking strategy can both be seen as a subgroup analysis (e.g., "evaluate data separately for men and women") or as tinkering with inclusion criteria (e.g., "include only women (if the analysis is significant for them)"). Technically, for tinkering with inclusion criteria, maybe only one of the levels of the subgroups should be evaluated but this would imply that the selection of the inclusion criterion was theory-driven. The current approach does not assume this (inclusion criterion is "HARKed").

The simulation code can be found in the file `subgroupAnalysis.R`. 

**Example**
```{r subgroupAnalysis}
set.seed(1234)
sim.subgroupHack(nobs.group = 30, nsubvars = 3, alternative = "two.sided", 
                 strategy = "smallest", alpha = 0.05, iter = 10)
```

### Composite scores / scale redefinition
We simulate this p-hacking strategy using univariate linear regression (so that we don't mix it up with the multiple dependent variable p-hacking strategy). The **simulated dataset** consists of one dependent variable and several correlated score variables that can potentially form the items of a score (these are not correlated with the dependent variable). The correlation between these score variables can be specified using the `rcomp` argument, the number of the score variables can be specified using the `ncompv` argument. The **p-hacking strategy** is implemented as follows: First, the original p-value is calculated using the mean score of all score variables as a predictor variable. Then, one by one, items are removed from the scale. The next item to remove is always chosen based on "Cronbach's alpha when item deleted" (the common choice in SPSS), i.e., the item is chosen that makes the scale most consistent. In each iteration, the  p-values for linear regressions using the following predictors are calculated: (1) new composite score, i.e., reduced item scale; (2) deleted item from the score; (3) a composite score of all items that have been deleted so far. The maximum number of items to be removed from the scale can be set using the argument `ndelete`. In the end, the final p-value is selected using one of the p-value selection strategies (first significant p-value, smallest significant p-value, smallest p-value). The output contains the original and p-hacked p-values as well as the original and p-hacked $R^2$.

The simulation code can be found in the file `compositeScores.R`.

**Example**
```{r compositeScores}
set.seed(1234)
sim.compscoreHack(nobs = 30, ncompv = 5, rcomp = 0.7, ndelete = 3, 
                  strategy = "smallest", alpha = 0.05, iter = 10)
```

### Selective reporting of effects
We simulate this p-hacking strategy using linear regression because we want to keep it separate from the "exploiting covariates" strategy and because in linear regression all effects are potentially interesting. The **simulated dataset** consists of one continuous dependent variable and several continuous correlated predictor variables (independent variables). The correlation between the predictor variables and the dependent variable is set to zero. The correlation between the predictor variables can be defined using the argument `riv`. The number of predictor variables can be specified using the argument `niv`. The **p-hacking strategy** is implemented as follows: A multiple linear regression is calculated using all predictor variables and all p-values apart from the intercept are saved. The first of these p-values (effect of the first predictor on the dependent variable) is assumed to be the original p-value (this implies a hierarchy between the predictor variables). The final p-value is selected using one of the p-value selection strategies (first significant p-value, smallest significant p-value, smallest p-value). The user can specify whether the model should contain interactions (argument `interactions = FALSE` or `interactions = TRUE`, default: FALSE). This increases the number of available p-values (however, when interaction terms are used, the sample size needs to be large so that the model is still identified). The output contains the original and p-hacked p-values as well as the original and p-hacked $R^2$.

The simulation code can be found in the file `selectEffects.R`.

**Example**
```{r selectEffects}
set.seed(1234)
sim.selectEffects(nobs = 30, niv = 4, riv = 0.1, interactions = FALSE, 
                  strategy = "smallest", alpha = 0.05, iter = 10)
```

### Exploiting variable transformations
We simulate this p-hacking strategy using univariate linear regression because most variable transformation strategies are not applicable to discrete variables. The **simulated dataset** contains two continuous variables that are not correlated with each other. The **p-hacking strategy** is implemented as follows: The original p-value is calculated as a simple univariate linear regression y ~ x. Then, the variables in the model are transformed. Using the argument `transvar` one can specify whether only x (`transvar = "x"`), only y (`transvar = "y"`) or both (`transvar = "xy"`) should be transformed. The respective variables are transformed using a log transformation, a square root transformation and the inverse. Then, linear regressions are computed for all transformations of the variables. The final p-value is selected using one of the p-value selection strategies (first significant p-value, smallest significant p-value, smallest p-value). The output contains the original and p-hacked p-values as well as the original and p-hacked $R^2$.

The simulation code can be found in the file `variableTransformation.R`.

**Example**
```{r variableTransformation}
set.seed(1234)
sim.varTransHack(nobs = 30, transvar = "xy", strategy = "smallest", 
                 alpha = 0.05, iter = 10)
```

### Exploiting arbitrary cutoff values
We simulate this p-hacking strategy using a t-test / ANOVA (but the underlying data is actually continuous). The **simulated dataset** contains two continuous variables that are uncorrelated (dependent and independent variable). In the **p-hacking strategy**, three mechanisms are applied to make the independent variable discrete. The first mechanism is a median split (leading to two groups, ergo a t-test). The second mechanism is a three-wise split, after which only the two extreme categories are compared in a t-test ("cut the middle" strategy). The third mechanism is a three-wise split again, but this time all three categories are evaluated using an ANOVA (only the omnibus test is evaluated). Additionally to these, the original univariate linear regression is evaluated to obtain the original p-value. Then the final p-value is selected using one of the p-value selection strategies (first significant p-value, smallest significant p-value, smallest p-value). The output contains the original and p-hacked p-values as well as the original and p-hacked $R^2$.

The simulation code can be found in the file `exploitCutoffs.R`.

**Example**
```{r exploitCutoffs}
set.seed(1234)
sim.cutoffHack(nobs = 30, strategy = "smallest", alpha = 0.05, iter = 10)
```

### Exploiting statistical analysis options
We simulate this p-hacking strategy using a t-test (and related tests for comparing the central tendency in two independent groups). The **simulated dataset** contains a discrete group variable (independent variable) and a continuous dependent variable. The **p-hacking strategy** is implemented as follows: The central tendency in the groups is compared using a t-test (original analysis), a Welch test, a Wilcoxon test, and a Yuen test (trimming options: 0.1, 0.15, 0.2, and 0.25). Then the final p-value is selected using one of the p-value selection strategies (first significant p-value, smallest significant p-value, smallest p-value). The output contains only the original and p-hacked p-values because there is no common effect size for the analayses used.

The simulation code can be found in the file `statAnalysis.R`.

**Example**
```{r statAnalysis}
set.seed(1234)
sim.statAnalysisHack(nobs.group = 30, strategy = "smallest", 
                     alternative = "two.sided", alpha = 0.05, iter = 10)
```

### Favorable imputation of missing values
We simulate this p-hacking strategy using univariate linear regression because many imputation methods have been developed for continuous variables. The **simulated dataset** contains two non-correlated continuous variables. Both of these contain missing values. The percentage of missing values is defined by the argument `missing`. The **p-hacking strategy** is implemented similar to the outlier exclusion strategy. There are 10 different imputation methods that can be selected using the argument `which`. It is possible to select all of them (`which = c(1:10)`), a specific subset of them (e.g., `which = c(1,4,6)`), or a random subset of 5 imputation methods (`which = "random"`). In the end, the final p-value is selected using one of the p-value selection strategies (first significant p-value, smallest significant p-value, smallest p-value). The output contains the original and p-hacked p-values as well as the original and p-hacked $R^2$.

Below, we describe all imputation methods:

1. Delete missing values: Pairwise deletion of missing values (default option of lm()).
1. Mean imputation: Missing values are replaced by the mean of the variable.
1. Median imputation: Missing values are replaced by the median of the variable.
1. Mode imputation: Missing values are replaced by the mode of the variable.
1. Predictive mean matching: Missing values are imputed using the "pmm" method from the `mice` R-package.
1. Weighted predictive mean matching: Missing values are imputed using the "midastouch" method from the `mice` R-package.
1. Sample from observed values: Missing values are imputed using the "sample" method from the `mice` R-package.
1. Bayesian linear regression: Missing values are imputed using the "norm" method from the `mice` R-package.
1. Linear regression ignoring model error: Missing values are imputed using the "norm.nob" method from the `mice` R-package.
1. Linear regression predicted values: Missing values are imputed using the "norm.predict" method from the `mice` R-package.

The simulation code can be found in the file `favorableImputation.R`.

**Example**
```{r favorableImputation}
set.seed(1234)
sim.impHack(nobs = 30, missing = 0.2, which = c(1:10), strategy = "smallest", 
            alpha = 0.05, iter = 10)
```
